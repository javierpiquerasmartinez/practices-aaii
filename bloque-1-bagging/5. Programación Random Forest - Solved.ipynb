{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"#004D7F\" size=5>Módulo 2: Bootstrap Aggregation</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=6>2. Programación Random Forest </font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Manuel Castillo-Cara</font><br>\n",
    "<font color=\"#004D7F\" size=3>Aprendizaje Automático II</font><br>\n",
    "<font color=\"#004D7F\" size=3>Universidad Nacional de Educación a Distancia</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "   * [1.1. Algoritmo Random Forest](#section11)\n",
    "   * [1.2. Dataset](#section12)\n",
    "   * [1.3. Tutorial](#section13)\n",
    "* [2. Calcular las divisiones](#section2)\n",
    "* [3. Caso de estudio: dataset Sonar](#section3)\n",
    "   * [3.1. Resultados](#section31)\n",
    "* [Ejercicios](#sectionEj)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\">0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles de decisión pueden sufrir de gran varianza, lo que hace que sus resultados sean frágiles para los datos de entrenamiento específicos utilizados. La creación de múltiples modelos a partir de muestras de sus datos de entrenamiento, lo que se denomina bagging, puede reducir esta varianza, pero los árboles están altamente correlacionados.\n",
    "\n",
    "Random Forest es una extensión bagging que, además de construir árboles basándose en múltiples muestras de sus datos de entrenamiento, también restringe las características que se pueden usar para construir los árboles, lo que obliga a los árboles a ser diferentes. Esto, a su vez, puede aumentar el rendimiento. En este tutorial, descubrirá cómo implementar el algoritmo Random Forest desde cero en Python. Después de completar este tutorial, sabrá:\n",
    "- La diferencia entre árboles de decisión en bagging y el algoritmo Random Forest.\n",
    "- Cómo construir árboles de decisión en bagging con más variación.\n",
    "- Cómo aplicar el algoritmo de bosque aleatorio a un problema de modelado predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\"> 1. Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección proporciona una breve descripción de Bootstrap Aggregation y el conjunto de datos de Sonar que se utilizarán en este tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section11\"></a>\n",
    "## <font color=\"#004D7F\"> 1.1. Algoritmo Random Forest</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles de decisión implican la selección codiciosa del mejor punto de división del conjunto de datos en cada paso. Este algoritmo hace que los árboles de decisión sean susceptibles a una gran variación si no se podan. Esta alta varianza se puede aprovechar y reducir creando múltiples árboles con diferentes muestras del conjunto de datos de entrenamiento (diferentes vistas del problema) y combinando sus predicciones. Este enfoque se llama bootstrap aggregation o bagging para abreviar.\n",
    "\n",
    "Una limitación de bagging es que se utiliza el mismo algoritmo codicioso para crear cada árbol, lo que significa que es probable que se elijan puntos de división iguales o muy similares en cada árbol, lo que hace que los diferentes árboles sean muy similares (los árboles estarán correlacionados). Esto, a su vez, hace que sus predicciones sean similares, mitigando la variación buscada originalmente.\n",
    "\n",
    "Podemos forzar que los árboles de decisión sean diferentes limitando las características (columnas) que el\n",
    "algoritmo codicioso puede evaluar en cada punto de división al crear el árbol. Esto se llama algoritmo de Random Forest. \n",
    "- Al igual que el bagging, se toman varias muestras del conjunto de datos de entrenamiento y se\n",
    "entrena un árbol diferente en cada una. \n",
    "- La diferencia es que en cada punto se realiza una división de los datos y se agrega al árbol; solo se puede considerar un subconjunto fijo de atributos. \n",
    "- Para problemas de clasificación, el tipo de problemas que veremos en este tutorial, la cantidad de atributos que se considerarán para la división se limita a la raíz cuadrada de la cantidad de características de entrada.\n",
    "$$\n",
    "       num\\_features\\_for\\_split = \\sqrt{total\\_input\\_features}\n",
    "$$\n",
    "\n",
    "El resultado de este pequeño cambio son árboles que son más diferentes entre sí (no correlacionados), lo que da como resultado predicciones más diversas y una predicción combinada que a menudo tiene mejor rendimiento que un solo árbol o el bagging por sí solo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__: En Scikit-learn podemos ver los diferentes parámetros que tiene [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) y [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section12\"></a>\n",
    "## <font color=\"#004D7F\"> 1.2. Dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial usaremos el conjunto de datos Sonar. \n",
    "- Este conjunto de datos implica la discriminación entre minas y rocas. \n",
    "- El desempeño de referencia sobre el problema es aproximadamente del 53%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> __Nota__: Más información sobre el dataset [Sonar](https://archive.ics.uci.edu/dataset/151/connectionist+bench+sonar+mines+vs+rocks)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section13\"></a>\n",
    "## <font color=\"#004D7F\"> 1.3. Tutorial</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial se divide en 2 partes:\n",
    "- Cálculo de divisiones.\n",
    "- Caso de estudio Sonar.\n",
    "Estos pasos proporcionan la base que necesita para implementar y aplicar Random Forest a sus propios problemas de modelado predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a> \n",
    "# <font color=\"#004D7F\"> 2. Calcular divisiones</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un árbol de decisión, los puntos de división se eligen encontrando el atributo y el valor de ese atributo que resulta en el costo más bajo. \n",
    "- Para problemas de clasificación, esta función de costos suele ser el índice de Gini que calcula la pureza de los grupos de datos creados por el punto de división. \n",
    "- Un índice de Gini de 0 es una pureza perfecta donde los valores de clase están perfectamente separados en dos grupos, en el caso de un problema de clasificación de dos clases.\n",
    "- Encontrar el mejor punto de división en un árbol de decisión implica evaluar el costo de cada valor en el conjunto de datos de entrenamiento para cada variable de entrada. \n",
    "\n",
    "Para Bagging y bosque aleatorio, este procedimiento se ejecuta sobre una muestra del conjunto de datos de entrenamiento, realizado con reemplazo. El muestreo con reemplazo significa que se puede elegir la misma fila y agregarla a la muestra más de una vez.\n",
    "\n",
    "Podemos actualizar este procedimiento para Random Forest. \n",
    "- En lugar de enumerar todos los valores de los atributos de entrada en busca de la división con el costo más bajo, podemos crear una muestra de los atributos de entrada a considerar. \n",
    "- Esta muestra de atributos de entrada se puede elegir aleatoriamente y sin reemplazo, lo que significa que cada atributo de entrada solo debe considerarse una vez cuando se busca el punto de división con el costo más bajo.\n",
    "\n",
    "Veamos código:\n",
    "- A continuación se muestra el nombre de la función `get_split()` que implementa este procedimiento. \n",
    "- Se necesita un conjunto de datos y una cantidad fija de características de entrada para evaluar como argumentos de entrada, donde el conjunto de datos puede ser una muestra del conjunto de datos de entrenamiento real. \n",
    "- La función auxiliar `test_split()` se usa para dividir el conjunto de datos por un punto de división candidato y\n",
    "- El `gini_index()` se usa para evaluar el costo de una división determinada por los grupos de filas creados.\n",
    "- Podemos ver que se crea una lista de características seleccionando aleatoriamente índices de características y agregándolos a una lista (llamadas `features`), \n",
    "- Luego esta lista de características se enumera y los valores específicos en el conjunto de datos de entrenamiento se evalúan como puntos de división."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfeatures = list()\n",
    "\twhile len(features) < n_features:\n",
    "\t\tindex = randrange(len(dataset[0])-1)\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features:\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos cómo se puede modificar un algoritmo de árbol de decisión para usarlo con el algoritmo Random Forest, podemos unirlo con una implementación de bagging y aplicarlo a un conjunto de datos del mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a> \n",
    "# <font color=\"#004D7F\"> 3. Caso de estudio: dataset Sonar</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, aplicaremos el algoritmo Random Forest al conjunto de datos de Sonar. \n",
    "1. Primero se carga el conjunto de datos, los valores de cadena se convierten a numéricos y la columna de salida se convierte de cadenas a valores enteros de 0 y 1. \n",
    "    - Esto se logra con las funciones auxiliares `load_csv()`, `str_column_to_float()` y `str_column_to_int()` para cargar y preparar el conjunto de datos.\n",
    "2. Utilizaremos _k_-fold para estimar el rendimiento del modelo aprendido en datos invisibles. \n",
    "    - Esto significa que construiremos y evaluaremos _k_ modelos y estimaremos el rendimiento como el error medio del modelo. \n",
    "    - El accuracy de la clasificación se utilizará para evaluar cada modelo. \n",
    "    - Estos comportamientos se proporcionan en las funciones `cross_valiation_split()`, `accuracy_metric()` y `evaluate_algorithm()`.\n",
    "3. También usaremos una implementación del algoritmo CART\n",
    "adaptado para bagging y las funciones auxiliares del Capítulo 11 , incluyendo: \n",
    "    - `test split()` para dividir un conjunto de datos en grupos, \n",
    "    - `gini_index()` para evaluar un punto de división, \n",
    "    - `get_split()` para encontrar un punto de división óptimo, \n",
    "    - `to_terminal()`, `split()` y `build_tree()` se usan para crear un único árbol de decisión, \n",
    "    - `predict()` para hacer una predicción con un árbol de decisión y\n",
    "    - `subsample()` descrita en el paso anterior para hacer una submuestra del conjunto de datos de entrenamiento.\n",
    "    - `bagging_predict()` para hacer una predicción con una lista de árboles de decisión.\n",
    "5. Se desarrolla una nueva función llamada `random_forest()` que primero crea una lista de árboles de decisión a partir de submuestras del conjunto de datos de entrenamiento y luego los utiliza para hacer predicciones.\n",
    "    - Como dijimos anteriormente, la diferencia clave entre Random Forest y los árboles de decisión en bagged es el único pequeño cambio en la forma en que se crean los árboles aquí en la función `get_split()`.\n",
    "\n",
    "El ejemplo completo se enumera a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: [56.09756097560976, 63.41463414634146, 60.97560975609756, 58.536585365853654, 73.17073170731707]\n",
      "Mean Accuracy: 62.439%\n",
      "Trees: 5\n",
      "Scores: [70.73170731707317, 58.536585365853654, 85.36585365853658, 75.60975609756098, 63.41463414634146]\n",
      "Mean Accuracy: 70.732%\n",
      "Trees: 10\n",
      "Scores: [75.60975609756098, 80.48780487804879, 92.6829268292683, 73.17073170731707, 70.73170731707317]\n",
      "Mean Accuracy: 78.537%\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Algorithm on Sonar Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor _ in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left, n_features)\n",
    "\t\tsplit(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right, n_features)\n",
    "\t\tsplit(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "\troot = get_split(train, n_features)\n",
    "\tsplit(root, max_depth, min_size, n_features, 1)\n",
    "\treturn root\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(dataset) * ratio)\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(dataset))\n",
    "\t\tsample.append(dataset[index])\n",
    "\treturn sample\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "\tpredictions = [predict(tree, row) for tree in trees]\n",
    "\treturn max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "\ttrees = list()\n",
    "\tfor _ in range(n_trees):\n",
    "\t\tsample = subsample(train, sample_size)\n",
    "\t\ttree = build_tree(sample, max_depth, min_size, n_features)\n",
    "\t\ttrees.append(tree)\n",
    "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
    "\treturn(predictions)\n",
    "\n",
    "# Test the random forest algorithm on sonar dataset\n",
    "seed(2)\n",
    "# load and prepare data\n",
    "filename = 'data/sonar.all-data.csv'\n",
    "dataset = load_csv(filename)\n",
    "# convert string attributes to integers\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "for n_trees in [1, 5, 10]:\n",
    "\tscores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "\tprint('Trees: %d' % n_trees)\n",
    "\tprint('Scores: %s' % scores)\n",
    "\tprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a> \n",
    "## <font color=\"#004D7F\"> 3.1. Resultados</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se utilizó un valor _k_ de 5 para la validación cruzada, dando a cada pliegue $\\frac{208}{5} = 41.6$ o justo 40 registros que se evaluarán en cada iteración.\n",
    "- Se construyeron árboles con una profundidad máxima de 10 y un número mínimo de filas de entrenamiento en cada nodo de 1. \n",
    "- Se crearon muestras del conjunto de datos de entrenamiento con el mismo tamaño que el conjunto de datos original, que es una expectativa predeterminada para el algoritmo Random Forest.\n",
    "- El número de características consideradas en cada punto de división se estableció en $\\sqrt{60} = 7,74$ redondeado a 7 características. \n",
    "- Se evaluó un conjunto de 3 números diferentes de árboles para comparar, lo que muestra la habilidad cada vez mayor a medida que se agregan más árboles. \n",
    "- Al ejecutar el ejemplo se imprimen las puntuaciones de cada pliegue y la puntuación media de cada configuración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sectionEj\"></a>\n",
    "<h3><font color=\"#004D7F\" size=6> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicios</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se proponen las siguientes actividades para consolidar el aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#004D7F\" size=5>Ejercicio 1</font>\n",
    "__Ajuste de algoritmos__. La configuración utilizada en el tutorial se encontró con un poco de prueba y error pero no estaba optimizada. Experimente con una mayor cantidad de árboles, diferentes cantidades de características e incluso diferentes configuraciones de árboles para mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#004D7F\" size=5>Ejercicio 2</font>\n",
    "__Más problemas__. Aplicar la técnica a otros problemas de clasificación e incluso adaptarla para regresión con una nueva función de costos y un nuevo método para combinar las predicciones de los árboles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
